#!/usr/bin/env python3
"""
Test ML/AI Discipline Implementation

Test alle aspecten van ML discipline:
- Dataset versioning met hash-based tracking
- Model registry met metadata en lifecycle management
- Evaluation metrics tracking en drift detection
- Canary deployment met â‰¤1% risk budget
- Automated rollback capabilities
"""

import asyncio
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
import logging

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

from src.cryptosmarttrader.ml.model_registry import get_model_registry, ModelStatus
from src.cryptosmarttrader.ml.canary_deployment import get_canary_orchestrator, CanaryConfig, CanaryPhase


def generate_sample_dataset(n_samples=1000, n_features=10):
    """Generate sample dataset voor testing"""
    
    # Create features
    X = np.random.randn(n_samples, n_features)
    
    # Create target with some signal
    weights = np.random.randn(n_features)
    y = (X @ weights + np.random.randn(n_samples) * 0.1) > 0
    
    # Convert to DataFrame
    feature_names = [f"feature_{i}" for i in range(n_features)]
    df = pd.DataFrame(X, columns=feature_names)
    df['target'] = y.astype(int)
    
    return df


def train_sample_model(X_train, y_train, X_test, y_test):
    """Train sample model en compute metrics"""
    
    # Train RandomForest
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    # Validation metrics
    y_val_pred = model.predict(X_train)
    validation_metrics = {
        'accuracy': accuracy_score(y_train, y_val_pred),
        'precision': precision_score(y_train, y_val_pred, average='weighted'),
        'recall': recall_score(y_train, y_val_pred, average='weighted'),
        'f1': f1_score(y_train, y_val_pred, average='weighted')
    }
    
    # Test metrics
    y_test_pred = model.predict(X_test)
    test_metrics = {
        'accuracy': accuracy_score(y_test, y_test_pred),
        'precision': precision_score(y_test, y_test_pred, average='weighted'),
        'recall': recall_score(y_test, y_test_pred, average='weighted'),
        'f1': f1_score(y_test, y_test_pred, average='weighted')
    }
    
    return model, validation_metrics, test_metrics


def test_dataset_versioning():
    """Test dataset versioning en hashing"""
    
    print("ğŸ§ª Testing Dataset Versioning...")
    
    registry = get_model_registry()
    
    # Generate en register eerste dataset versie
    df1 = generate_sample_dataset(1000, 10)
    dataset_v1 = registry.register_dataset(
        data=df1,
        dataset_id="crypto_signals",
        version="v1.0",
        transformations=["normalize", "feature_engineering"]
    )
    
    print(f"  ğŸ“Š Dataset v1.0 registered: {dataset_v1.hash[:8]}...")
    print(f"     - Rows: {dataset_v1.size_rows}")
    print(f"     - Features: {dataset_v1.feature_count}")
    print(f"     - Completeness: {dataset_v1.completeness_score:.3f}")
    print(f"     - Consistency: {dataset_v1.consistency_score:.3f}")
    print(f"     - Validity: {dataset_v1.validity_score:.3f}")
    
    # Generate tweede dataset versie (met wijzigingen)
    df2 = generate_sample_dataset(1200, 12)  # More samples, more features
    dataset_v2 = registry.register_dataset(
        data=df2,
        dataset_id="crypto_signals",
        version="v2.0",
        transformations=["normalize", "feature_engineering", "outlier_removal"]
    )
    
    print(f"  ğŸ“Š Dataset v2.0 registered: {dataset_v2.hash[:8]}...")
    print(f"     - Hash different from v1.0: {dataset_v1.hash != dataset_v2.hash}")
    
    return dataset_v1, dataset_v2


def test_model_registry():
    """Test model registry functionality"""
    
    print("\nğŸ¤– Testing Model Registry...")
    
    registry = get_model_registry()
    
    # Generate training data
    df = generate_sample_dataset(1000, 10)
    X = df.drop('target', axis=1)
    y = df['target']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Register dataset
    dataset_version = registry.register_dataset(df, "model_training_data", "v1.0")
    
    # Train model
    model, validation_metrics, test_metrics = train_sample_model(X_train, y_train, X_test, y_test)
    
    # Register model
    metadata = registry.register_model(
        model=model,
        model_id="crypto_predictor",
        name="Crypto Signal Predictor v1",
        algorithm="RandomForestClassifier",
        training_data_hash=dataset_version.hash,
        hyperparameters={'n_estimators': 100, 'random_state': 42},
        validation_metrics=validation_metrics,
        test_metrics=test_metrics,
        description="Initial crypto signal prediction model",
        tags=["crypto", "signals", "v1"]
    )
    
    print(f"  ğŸ¤– Model registered: {metadata.model_id} v{metadata.version}")
    print(f"     - Status: {metadata.status.value}")
    print(f"     - Test Accuracy: {metadata.test_metrics['accuracy']:.3f}")
    print(f"     - Test F1: {metadata.test_metrics['f1']:.3f}")
    
    # Test drift detection
    # Generate nieuwe data met slight drift
    drift_df = generate_sample_dataset(500, 10)
    drift_df.iloc[:, 0] += 2.0  # Add drift to first feature
    
    drift_status = registry.check_data_drift(
        model_id="crypto_predictor",
        version=metadata.version,
        new_data=drift_df,
        reference_data=df
    )
    
    print(f"  ğŸ” Drift Detection: {drift_status.value}")
    
    return metadata


async def test_canary_deployment():
    """Test canary deployment orchestration"""
    
    print("\nğŸš€ Testing Canary Deployment...")
    
    orchestrator = get_canary_orchestrator()
    
    # Create canary config
    config = CanaryConfig(
        model_id="crypto_predictor",
        version="20250814_120000",  # Simulate model version
        max_risk_budget_pct=1.0,
        paper_trading_days=1,  # Shortened for demo
        shadow_trading_days=1,
        canary_duration_hours=1,
        min_accuracy_threshold=0.7,
        max_drawdown_threshold=0.05,
        min_sharpe_threshold=0.8
    )
    
    print(f"  ğŸš€ Starting canary deployment: {config.model_id} v{config.version}")
    print(f"     - Max Risk Budget: {config.max_risk_budget_pct}%")
    print(f"     - Paper Trading: {config.paper_trading_days} days")
    print(f"     - Shadow Trading: {config.shadow_trading_days} days")
    print(f"     - Live Canary: {config.canary_duration_hours} hours")
    
    # Start canary deployment
    success = await orchestrator.start_canary_deployment(config)
    
    if success:
        print("  âœ… Canary deployment started successfully")
        
        # Monitor progress for demo (in production this runs in background)
        for i in range(5):
            await asyncio.sleep(2)  # Wait 2 seconds between checks
            
            status = orchestrator.get_canary_status(config.model_id, config.version)
            if status:
                print(f"     - Phase: {status.phase.value}")
                print(f"     - Trades: {status.total_trades}")
                print(f"     - Win Rate: {status.prediction_accuracy:.3f}")
                print(f"     - PnL: {status.total_pnl:.4f}")
                print(f"     - Risk Budget Used: {status.current_risk_budget_used:.2f}%")
                print(f"     - Healthy: {status.is_healthy}")
                
                if status.phase == CanaryPhase.FULL_PRODUCTION:
                    print("  ğŸ¯ Canary successfully promoted to production!")
                    break
                elif status.phase in [CanaryPhase.FAILED, CanaryPhase.ROLLING_BACK]:
                    print(f"  âŒ Canary failed or rolling back: {status.phase.value}")
                    break
            else:
                print("     - Canary completed or failed")
                break
                
        # Get final summary
        summary = orchestrator.get_canary_summary()
        print(f"  ğŸ“Š Canary Summary:")
        print(f"     - Active Canaries: {summary['active_canaries']}")
        print(f"     - Total Risk Budget Used: {summary['total_risk_budget_used']:.2f}%")
        
    else:
        print("  âŒ Failed to start canary deployment")


def test_registry_summary():
    """Test registry overview en management"""
    
    print("\nğŸ“Š Testing Registry Summary...")
    
    registry = get_model_registry()
    
    # Get registry summary
    summary = registry.get_registry_summary()
    
    print(f"  ğŸ“ˆ Registry Overview:")
    print(f"     - Total Models: {summary['total_models']}")
    print(f"     - Total Datasets: {summary['total_datasets']}")
    print(f"     - Registry Size: {summary['registry_size_mb']:.2f} MB")
    print(f"     - Models by Status:")
    
    for status, count in summary['models_by_status'].items():
        print(f"       * {status}: {count}")


async def main():
    """Main test function"""
    
    print("ğŸ§ª Testing ML/AI Discipline Implementation")
    print("=" * 60)
    
    try:
        # Test 1: Dataset Versioning
        dataset_v1, dataset_v2 = test_dataset_versioning()
        
        # Test 2: Model Registry
        model_metadata = test_model_registry()
        
        # Test 3: Canary Deployment
        await test_canary_deployment()
        
        # Test 4: Registry Summary
        test_registry_summary()
        
        print("\n" + "=" * 60)
        print("âœ… All ML/AI Discipline tests completed successfully!")
        print("\nğŸ¯ Key Features Demonstrated:")
        print("   â€¢ Dataset versioning met SHA-256 hashing")
        print("   â€¢ Model metadata tracking en lifecycle management")
        print("   â€¢ Evaluation metrics per model versie")
        print("   â€¢ Data drift detection met automatic thresholds")
        print("   â€¢ Canary deployment met â‰¤1% risk budget enforcement")
        print("   â€¢ Automated rollback op performance degradation")
        print("   â€¢ Enterprise-grade model registry")
        
    except Exception as e:
        print(f"\nâŒ Test failed with error: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())