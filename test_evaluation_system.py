#!/usr/bin/env python3
"""
Test script voor de complete evaluation system

Test beide components:
1. Daily Labeling Job - objectieve labels genereren
2. Daily Evaluator - comprehensive metrics & attribution analysis
"""

import sys
import json
from pathlib import Path
from datetime import datetime

# Set up paths
sys.path.append('src')

from cryptosmarttrader.evaluation.daily_labeling_job import run_daily_labeling_job
from cryptosmarttrader.evaluation.daily_evaluator import run_daily_evaluation

def test_evaluation_system():
    """Test complete evaluation system"""
    
    print("ğŸ§ª Testing Complete Evaluation System")
    print("=" * 60)
    
    # Ensure test data exists
    print("ğŸ“‹ Step 1: Running simple recommendation ledger test to generate data...")
    import subprocess
    result = subprocess.run([sys.executable, "test_simple_recommendation_ledger.py"], 
                          capture_output=True, text=True)
    
    if result.returncode == 0:
        print("âœ… Test data generated successfully")
    else:
        print(f"âš ï¸ Warning: Test data generation had issues: {result.stderr}")
    
    print("\n" + "=" * 60)
    
    # Test 1: Daily Labeling Job
    print("ğŸ“Š Step 2: Testing Daily Labeling Job...")
    
    try:
        labeling_result = run_daily_labeling_job(days_back=30)
        
        print(f"âœ… Labeling Job Status: {labeling_result['status']}")
        print(f"ğŸ“ Recommendations Processed: {labeling_result.get('recommendations_processed', 0)}")
        print(f"ğŸ·ï¸ Labels Generated: {labeling_result.get('labels_generated', 0)}")
        print(f"âš¡ Execution Analyses: {labeling_result.get('execution_analyses', 0)}")
        
        if labeling_result.get('files_created'):
            print(f"ğŸ“ Files Created: {', '.join(labeling_result['files_created'])}")
        
        # Show summary if available
        if 'summary' in labeling_result:
            summary = labeling_result['summary']
            print(f"\nğŸ“ˆ Labeling Summary:")
            print(f"   Total Labels: {summary.get('total_labels_generated', 0)}")
            
            # Show horizon-specific stats
            horizon_stats = summary.get('label_stats_by_horizon', {})
            for horizon, stats in horizon_stats.items():
                print(f"   {horizon}: {stats.get('positive_rate', 0):.1%} positive rate, "
                      f"{stats.get('avg_forward_return_bps', 0):.1f} bps avg return")
    
    except Exception as e:
        print(f"âŒ Labeling Job Error: {e}")
    
    print("\n" + "=" * 60)
    
    # Test 2: Daily Evaluator
    print("ğŸ“Š Step 3: Testing Daily Evaluator...")
    
    try:
        evaluation_result = run_daily_evaluation()
        
        print(f"âœ… Evaluation Status: {evaluation_result['status']}")
        print(f"ğŸ“Š Samples Analyzed: {evaluation_result.get('samples_analyzed', 0)}")
        
        if evaluation_result.get('files_created'):
            print(f"ğŸ“ Files Created: {', '.join(evaluation_result['files_created'])}")
        
        # Show key performance metrics
        if 'summary' in evaluation_result:
            summary = evaluation_result['summary']
            
            print(f"\nğŸ“ˆ Performance Summary:")
            
            key_perf = summary.get('key_performance', {})
            if key_perf:
                print(f"   Hit Rate: {key_perf.get('hit_rate', 0):.1%}")
                print(f"   Avg PnL: {key_perf.get('avg_pnl_bps', 0):.1f} bps")
                print(f"   Total PnL: {key_perf.get('total_pnl_bps', 0):.1f} bps")
                print(f"   Sharpe Ratio: {key_perf.get('sharpe_ratio', 0):.3f}")
            
            risk_assess = summary.get('risk_assessment', {})
            if risk_assess:
                print(f"\nğŸ›¡ï¸ Risk Assessment:")
                print(f"   Max Drawdown: {risk_assess.get('max_drawdown_bps', 0):.1f} bps")
                print(f"   Volatility: {risk_assess.get('volatility_bps', 0):.1f} bps")
                print(f"   Information Ratio: {risk_assess.get('information_ratio', 0):.3f}")
            
            signal_quality = summary.get('signal_quality', {})
            if signal_quality:
                print(f"\nğŸ¯ Signal Quality:")
                print(f"   Score Correlation: {signal_quality.get('combined_score_correlation', 0):.3f}")
                print(f"   Top/Bottom Spread: {signal_quality.get('top_bottom_spread_bps', 0):.1f} bps")
            
            calibration = summary.get('model_calibration', {})
            if calibration:
                print(f"\nğŸ›ï¸ Model Calibration:")
                print(f"   Brier Score: {calibration.get('brier_score', 0):.4f}")
                print(f"   Log Loss: {calibration.get('log_loss', 0):.4f}")
                print(f"   Calibration Slope: {calibration.get('calibration_slope', 0):.3f}")
    
    except Exception as e:
        print(f"âŒ Evaluation Error: {e}")
    
    print("\n" + "=" * 60)
    
    # Test 3: Check output files
    print("ğŸ“ Step 4: Checking Output Files...")
    
    output_dirs = [
        Path("data/labels"),
        Path("data/evaluation"),
        Path("test_data")
    ]
    
    for output_dir in output_dirs:
        if output_dir.exists():
            files = list(output_dir.glob("*.csv")) + list(output_dir.glob("*.json"))
            if files:
                print(f"\nğŸ“‚ {output_dir}:")
                for file in sorted(files)[-3:]:  # Show last 3 files
                    size = file.stat().st_size
                    print(f"   {file.name} ({size} bytes)")
    
    print("\nâœ… All evaluation system tests completed!")
    print("\nKey Features Demonstrated:")
    print("â€¢ Objectieve labeling met forward-looking returns (6h & 24h horizons)")
    print("â€¢ No-lookahead policy enforcement")
    print("â€¢ Execution quality analysis (best vs actual entry)")
    print("â€¢ Comprehensive performance metrics (hit rate, precision@K, Sharpe)")
    print("â€¢ Risk-adjusted returns (Information Ratio, Sortino, Calmar)")
    print("â€¢ Calibration analysis (Brier score, log loss)")
    print("â€¢ Signal attribution per bucket (momentum, sentiment, whale)")
    print("â€¢ Regret analysis voor bandit-style evaluation")
    print("â€¢ Temporal performance trends")
    print("â€¢ Complete audit trail met JSON persistence")
    
    return True

if __name__ == "__main__":
    test_evaluation_system()